{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\n\n# Define device (GPU if available, else CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define transformations for data augmentation and normalization\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize images to 224x224\n    transforms.RandomHorizontalFlip(),  # Random horizontal flip for augmentation\n    transforms.RandomRotation(15),  # Random rotation for augmentation\n    transforms.ToTensor(),  # Convert images to tensors\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet means and std\n])\n\n# Load dataset (train and test)\ntrain_dataset = ImageFolder(root=\"/kaggle/input/grape-disease/grape_dataset/train\", transform=transform)\ntest_dataset = ImageFolder(root=\"/kaggle/input/grape-disease/grape_dataset/test\", transform=transform)\n\n# Create data loaders for batching\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# Load the VGG16 model pre-trained on ImageNet\nmodel = models.vgg16(pretrained=True)\n# Modify the last fully connected layer for 4 output classes (grape disease categories)\nmodel.classifier[6] = nn.Linear(in_features=4096, out_features=4)\nmodel = model.to(device)  # Move model to the defined device (GPU/CPU)\n\n# Define loss function (cross-entropy) and optimizer (Adam)\ncross_entropy_loss = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.00001)\n\n# Set number of epochs\nepochs = 10\n\n# Training loop over multiple epochs\nfor epoch in range(epochs):  \n    for i, batch in enumerate(train_loader, 0):\n        inputs, labels = batch\n        inputs = inputs.to(device)  # Move inputs to device\n        labels = labels.to(device)  # Move labels to device\n        optimizer.zero_grad()  # Zero the gradients to avoid accumulation\n        outputs = model(inputs)  # Forward pass\n        loss = cross_entropy_loss(outputs, labels)  # Calculate loss\n        loss.backward()  # Backpropagate the gradients\n        optimizer.step()  # Update the model parameters\n        print(loss)  # Print loss for every batch\n\n# Inspect predictions for the first batch from the test set\nimport pandas as pd\ninputs, labels = next(iter(test_loader))\ninputs = inputs.to(device)  # Move inputs to device\n\nlabels = labels.numpy()  # Convert labels to numpy\n# Get predictions from the model (output class with max score)\noutputs = model(inputs).max(1).indices.detach().cpu().numpy()\n\n# Calculate and print batch accuracy\ncomparison = pd.DataFrame()\nprint(\"Batch accuracy: \", (labels == outputs).sum() / len(labels))\ncomparison[\"labels\"] = labels\ncomparison[\"outputs\"] = outputs\ncomparison  # Display comparison of labels and predictions\n\n# Save the trained model's weights to a file\ntorch.save(model.state_dict(), \"/kaggle/working/fine_tuned_model.pth\")\nprint(\"Model saved successfully!\")\n\n# Install necessary libraries for explainability\n!pip install torch torchvision captum quantus grad-cam\n\n# Import necessary packages for explanation methods (e.g., Grad-CAM++, LRP)\nimport pathlib\nimport random\nimport copy\nimport gc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\nfrom torchvision import models\nfrom captum.attr import *\nimport quantus\nfrom pytorch_grad_cam import GradCAMPlusPlus\nfrom captum.attr import LRP\nfrom captum.attr._utils.lrp_rules import EpsilonRule, GammaRule, Alpha1_Beta0_Rule\n\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nimport cv2\n\n# Define class labels for your grape dataset (indices to class names)\nclass_labels = {0: \"Black Rot\", 1: \"Esca\", 2: \"Leaf Blight\", 3: \"Healthy\"}\n\n# Load the fine-tuned VGG16 model\nmodel = models.vgg16()\nmodel.classifier[6] = torch.nn.Linear(in_features=4096, out_features=4)  # Modify output for 4 classes\nmodel.load_state_dict(torch.load(\"/kaggle/input/fine-tuned-grape-leaf/fine_tuned_model.pth\", map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')))\nmodel.eval()  # Set the model to evaluation mode (no dropout, batch norm updates)\n\n# Define the image transformations for test data (same as training)\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load the test dataset\ntest_dataset = ImageFolder(root=\"/kaggle/input/grape-disease/grape_dataset/test\", transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n\n# Get a batch of test data\nx_batch, y_batch = next(iter(test_loader))\n\n# Initialize Grad-CAM++ for visualization\ncam = GradCAMPlusPlus(model=model, target_layers=[model.features[-1]])  # Target last convolutional layer\ngrayscale_cam = cam(input_tensor=x_batch)  # Generate Grad-CAM++ heatmaps\nlayers = list(model._modules[\"features\"]) + list(model._modules[\"classifier\"])  # Combine feature and classifier layers\nnum_layers = len(layers)\n\n# Initialize LRP (Layer-wise Relevance Propagation)\nfor idx_layer in range(1, num_layers):\n    if idx_layer <= 30:  # First few layers use Alpha1_Beta0_Rule\n        setattr(layers[idx_layer], \"rule\", Alpha1_Beta0_Rule())\n    elif idx_layer >= 31:  # Later layers use EpsilonRule\n        setattr(layers[idx_layer], \"rule\", EpsilonRule(epsilon=0))\n\nlrp = LRP(model)  # Initialize LRP method\n# Generate LRP attributions for the batch\nattributions_lrp = lrp.attribute(x_batch, target=y_batch)\nattributions_lrp_np = attributions_lrp.cpu().detach().numpy()  # Convert to numpy\n# Reorganize the attribution array for visualization\nattributions_lrp_np = np.transpose(attributions_lrp_np, (0, 2, 3, 1))\nattributions_lrp_np = np.average(attributions_lrp_np, axis=3)  # Average across channels\n# Normalize the attribution values\nattributions_lrp_np = (attributions_lrp_np.T / np.max(attributions_lrp_np, axis=(1, 2)).T).T\n\n# Process Grad-CAM heatmap (apply thresholding)\ngrayscale_cam_thr = np.copy(grayscale_cam)\ngrayscale_cam_thr = grayscale_cam_thr - 0.2  # Apply a threshold to remove weak activations\ngrayscale_cam_thr = np.clip(grayscale_cam_thr, 0, 1)  # Clip values between 0 and 1\n\n# Multiply Grad-CAM and LRP outputs to get a combined attribution map\nproduct = attributions_lrp_np * grayscale_cam_thr\nproduct = (product.T / np.max(product, axis=(1, 2)).T).T  # Normalize the combined map\n\n# Apply Gaussian smoothing to the combined attribution map\nsmoothed = np.zeros(product.shape)\nfor i in range(0, product.shape[0]):\n    smoothed[i] = cv2.GaussianBlur(product[i], (5, 5), cv2.BORDER_DEFAULT)\n    smoothed[i] = smoothed[i] / np.max(smoothed[i])  # Normalize after smoothing\n\n# Plotting the results (original image, Grad-CAM, LRP, and combined attribution map)\nfig, axes = plt.subplots(nrows=1, ncols=5, figsize=(15, 4.5))\naxes[0].imshow(np.moveaxis(quantus.normalise_func.denormalise(x_batch[1].cpu().detach().numpy(), mean=np.array([0.485, 0.456, 0.406]), std=np.array([0.229, 0.224, 0.225])), 0, -1), vmin=0.0, vmax=1.0)\naxes[0].title.set_text(f\"Input(class '{class_labels[y_batch[1].item()]}')\")\naxes[1].imshow(grayscale_cam[1], cmap=\"seismic\", vmin=-1.0, vmax=1.0)\naxes[1].title.set_text(\"Grad-CAM\")\naxes[2].imshow(grayscale_cam_thr[1], cmap=\"seismic\", vmin=-1.0, vmax=1.0)\naxes[2].title.set_text(\"Threshold created from Grad-CAM\")\naxes[3].imshow(attributions_lrp_np[1], cmap=\"seismic\", vmin=-1.0, vmax=1.0)\naxes[3].title.set_text(\"LRP\")\naxes[4].imshow(smoothed[1], cmap=\"seismic\", vmin=-1.0, vmax=1.0)\naxes[4].title.set_text(\"Proposed method\")\n\n# Remove axes ticks for better visualization\nfor i in range(0, 5):\n    axes[i].axis(\"on\")\n    axes[i].get_xaxis().set_ticks([])\n    axes[i].get_yaxis().set_ticks([])\n\n# Save the image with comparisons\nplt.savefig('/kaggle/working/example.png', bbox_inches='tight')\n\n# Save qualitative comparisons for each test image\nexplanations = {\n    \"GradCAM\": grayscale_cam,\n    \"LRP\": attributions_lrp_np,\n    \"Proposed method\": smoothed\n}\n\nfig, axes = plt.subplots(nrows=len(x_batch), ncols=1+len(explanations), figsize=(15, 4.5*len(x_batch)))\nfor index in range(0, len(x_batch)):\n    axes[index][0].imshow(np.moveaxis(quantus.normalise_func.denormalise(x_batch[index].cpu().detach().numpy(), mean=np.array([0.485, 0.456, 0.406]), std=np.array([0.229, 0.224, 0.225])), 0, -1), vmin=0.0, vmax=1.0)\n    axes[index][0].title.set_text(f\"Grape class {class_labels[y_batch[index].item()]}\")\n    axes[index][0].axis(\"off\")\n    for i, (k, v) in enumerate(explanations.items()):\n        axes[index][i+1].imshow(explanations[k][index] / np.max(explanations[k][index]), cmap=\"seismic\", vmin=-1.0, vmax=1.0)\n        axes[index][i+1].title.set_text(f\"{k}\")\n        axes[index][i+1].axis(\"off\")\n\n# Save the qualitative comparison images\nplt.savefig('/kaggle/working/qualitative.png', bbox_inches='tight')","metadata":{"_uuid":"77ae9bd6-46ff-41ea-9717-9147f23df575","_cell_guid":"fe9f4512-86f0-434e-8738-89fc01f030c2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from captum.metrics import infidelity\n\n# Dynamically match noise size with x_batch\n# This noise is used to perturb the input images during infidelity evaluation\nnoise = torch.tensor(np.random.normal(0, 0.003, x_batch.shape)).float().to(x_batch.device)\n\n# Define the perturbation function required by the Captum `infidelity` metric\n# It returns both the noise and the perturbed input (input - noise)\ndef perturb_fn(inputs):\n    perturbation = torch.tensor(np.random.normal(0, 0.003, inputs.shape)).float().to(inputs.device)\n    return perturbation, inputs - perturbation\n\n# ----------------------------- Infidelity Computation -----------------------------\n\n# Compute Infidelity for LRP-based attributions\ninfid = infidelity(model, perturb_fn, x_batch, attributions_lrp, normalize=True, target=y_batch)\nprint(infid)\n\n# Prepare Grad-CAM attributions to be the same shape as input (batch, 3, 224, 224)\ngrayscale_cam_rp = np.repeat(grayscale_cam[..., np.newaxis], 3, axis=-1)  # Add 3 channels\ngrayscale_cam_rp = np.transpose(grayscale_cam_rp, (0, 3, 1, 2))           # Change to (B, C, H, W)\ngrayscale_cam_rp = torch.tensor(grayscale_cam_rp).float().to(x_batch.device)\n\n# Compute Infidelity for Grad-CAM attributions\ninfid_gcam = infidelity(model, perturb_fn, x_batch, grayscale_cam_rp, normalize=True, target=y_batch)\nprint(infid_gcam)\n\n# Prepare proposed method's heatmaps for infidelity evaluation\nsmoothed_rp = np.repeat(smoothed[..., np.newaxis], 3, axis=-1)\nsmoothed_rp = np.transpose(smoothed_rp, (0, 3, 1, 2))  # Convert to (B, C, H, W)\nsmoothed_rp = torch.tensor(smoothed_rp).float().to(x_batch.device)\n\n# Compute Infidelity for Proposed Method\ninfid_smoothed = infidelity(model, perturb_fn, x_batch, smoothed_rp, normalize=True, target=y_batch)\nprint(infid_smoothed)\n\n# ----------------------------- Visualization -----------------------------\n\n# Create x-axis for plotting\nx = np.arange(1, len(infid) + 1)\n\n# Convert tensors to numpy arrays for plotting\ninfid = infid.cpu().detach().numpy() if isinstance(infid, torch.Tensor) else infid\ninfid_gcam = infid_gcam.cpu().detach().numpy() if isinstance(infid_gcam, torch.Tensor) else infid_gcam\ninfid_smoothed = infid_smoothed.cpu().detach().numpy() if isinstance(infid_smoothed, torch.Tensor) else infid_smoothed\n\n# Sanity check to ensure same number of samples in all metrics\nassert len(infid) == len(x)\nassert len(infid_gcam) == len(x)\nassert len(infid_smoothed) == len(x)\n\n# Plot the infidelity scores for comparison\nfig, ax = plt.subplots()\nax.plot(x, infid, label='LRP', marker='o')\nax.plot(x, infid_gcam, label='GradCAM', marker='s')\nax.plot(x, infid_smoothed, label='Proposed', marker='^')\n\n# Formatting the plot\nax.legend()\nax.set_xlabel('Image Sample Index')\nax.set_ylabel('Infidelity Score')\nax.set_title('Infidelity Scores Across Samples')\nplt.grid(True)\nplt.savefig('/kaggle/working/infidelity_graph.png', bbox_inches='tight')\nplt.show()\n\n# ----------------------------- Quantus Metric Evaluation -----------------------------\n\n# Define the XAI methods and evaluation metrics from Quantus library\nxai_methods = ['Proposed_method', 'GradCAM', 'LRP']\nmetrics = {\n    \"Robustness\": quantus.AvgSensitivity(...),  # Measures attribution stability to small perturbations\n    \"Faithfulness\": quantus.FaithfulnessCorrelation(...),  # Correlates attribution with prediction impact\n    \"Localisation\": quantus.RelevanceRankAccuracy(...),  # Measures how accurately attributions localize relevant features\n    \"Complexity\": quantus.Sparseness(...),  # Measures sparsity (less complex = more interpretable)\n    \"Randomisation\": quantus.RandomLogit(...),  # Tests sensitivity of explanations to randomization of model\n}\n\nmodel.eval()  # Set model to evaluation mode\n\n# Prepare LRP attributions for use as segmentation masks (binary masks)\ns_batch = attributions_lrp.cpu().detach().numpy()\nthreshold = np.percentile(s_batch, 90)  # Keep top 10% of values as relevant\ns_batch = (s_batch >= threshold).astype(np.uint8)\ns_batch = np.expand_dims(s_batch, axis=1)  # Add channel dimension for Quantus compatibility\n\nprint(f\"s_batch shape: {s_batch.shape}, unique values: {np.unique(s_batch)}\")\n\n# Dictionary to store metric results for each XAI method\nresults = {method: {} for method in xai_methods}\n\n# Loop over each method and compute all metrics\nfor method in xai_methods:\n    for metric, metric_func in metrics.items():\n        print(f\"Evaluating {metric} of {method} method.\")\n\n        # Extract dimensions from input images\n        batch_size, channels, height, width = x_batch.shape\n\n        # Regenerate the segmentation masks to prevent reuse of previous s_batch\n        s_batch = attributions_lrp.cpu().detach().numpy()\n        threshold = np.percentile(s_batch, 90)\n        s_batch = (s_batch >= threshold).astype(np.uint8)\n        s_batch = np.expand_dims(s_batch, axis=1)\n\n        # Resize masks if necessary to match input image resolution\n        if s_batch.shape[2:] != (height, width):\n            from skimage.transform import resize\n            s_batch = np.array([resize(mask[0], (height, width), anti_aliasing=True) for mask in s_batch])\n            s_batch = np.expand_dims(s_batch, axis=1)\n            s_batch = (s_batch > 0.5).astype(np.uint8)  # Convert back to binary\n\n        print(f\"x_batch shape: {x_batch.shape}, s_batch shape: {s_batch.shape}, unique values: {np.unique(s_batch)}\")\n\n        # Call Quantus metric evaluation\n        scores = metric_func(\n            model=model,\n            x_batch=x_batch.cpu().detach().numpy(),\n            y_batch=y_batch.cpu().detach().numpy(),\n            a_batch=None,  # Not using continuous attributions\n            s_batch=s_batch,\n            explain_func=explainer_wrapper,  # Custom function to generate explanations per method\n            explain_func_kwargs={\"method\": method},\n        )\n        results[method][metric] = scores\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}